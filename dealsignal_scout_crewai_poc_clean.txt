{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd4c6137",
      "metadata": {},
      "source": [
        "# DealSignal Scout (CrewAI)\n",
        "## Prepared by Husayn El Sharif (Upwork PoC)\n",
        "\n",
        "This notebook uses a CrewAI multi-agent AI framework (Researcher → Analyst → Writer) to produce a **deal-signal monitoring agent**.\n",
        "\n",
        "### What it does\n",
        "- Pulls items from **1–2 external sources** (RSS by default; NewsAPI optional)\n",
        "- Uses an LLM to **detect deal signals**, tag them, and score relevance\n",
        "- Stores results in **JSON** \n",
        "- Prints a **daily summary** (CLI-style output)\n",
        "\n",
        "### Notes\n",
        "- Simple Proof-of-Concept: 1–2 feeds, short prompts, lightweight storage.\n",
        "- Options to add later: Slack/email alerts, more sources, scheduling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b54eaba",
      "metadata": {},
      "source": [
        "## 0) Install / Dependencies (if needed)\n",
        "If you're running this in a fresh environment, install the minimal packages below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4afa2097",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If needed (uncomment):\n",
        "# !pip -q install crewai langchain-openai python-dotenv feedparser requests pandas\n",
        "\n",
        "# Use environment: dlai001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bbc53d7",
      "metadata": {},
      "source": [
        "## 1) Imports + Environment Variables\n",
        "This mirrors your original notebook: load `.env`, configure LLM(s), keep code clean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef72601",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import sqlite3\n",
        "import hashlib\n",
        "import datetime as dt\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import feedparser\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from crewai import Agent, Crew, Task, Process\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284b6004",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR = Path(\"output\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28fa90e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables from .env\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found. Add it to your .env file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71f4be0f",
      "metadata": {},
      "source": [
        "## 2) Config\n",
        "These are your **configurable criteria** (industries, signal types, score threshold, etc.).\n",
        "Edit these values to match a client’s needs during a demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d63f487",
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # Sources: 1–2 external sources are ideal for the POC\n",
        "    \"rss_feeds\": [\n",
        "        \"https://techcrunch.com/feed/\",\n",
        "        \"https://www.prnewswire.com/rss/news-releases-list.rss\",\n",
        "    ],\n",
        "\n",
        "    # Optional: NewsAPI (set NEWSAPI_KEY in .env if you want)\n",
        "    \"use_newsapi\": False,\n",
        "    \"newsapi_query\": \"acquisition OR merger OR funding OR strategic alternatives\",\n",
        "    \"newsapi_language\": \"en\",\n",
        "    \"newsapi_page_size\": 20,\n",
        "\n",
        "    # Deal criteria (configurable)\n",
        "    \"industries\": [\"SaaS\", \"AI\", \"Healthcare\", \"Energy\", \"Industrial\"],\n",
        "    \"signal_types\": [\"M&A\", \"Funding\", \"Partnership\", \"Distress\", \"Regulatory\"],\n",
        "    \"min_relevance_score\": 0.70,\n",
        "\n",
        "    # Runtime / limits\n",
        "    \"max_items_per_source\": 15,     # per feed\n",
        "    \"max_total_items\": 30,          # optional cap across all sources (helps control cost)\n",
        "\n",
        "    # Output\n",
        "    \"output_dir\": \"output\",\n",
        "\n",
        "    # Run label used in filenames\n",
        "    \"run_label\": dt.datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e4643a",
      "metadata": {},
      "source": [
        "## 3) Utility: Fetch External Data (RSS + optional NewsAPI)\n",
        "This is the \"Monitor\" portion of the job scope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e93b4ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _hash_id(*parts: str) -> str:\n",
        "    raw = \"||\".join([p or \"\" for p in parts])\n",
        "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()[:24]\n",
        "\n",
        "def fetch_rss_items(feeds: List[str], max_items_per_feed: int = 15) -> List[Dict[str, Any]]:\n",
        "    items: List[Dict[str, Any]] = []\n",
        "    for feed_url in feeds:\n",
        "        parsed = feedparser.parse(feed_url)\n",
        "        for entry in parsed.entries[:max_items_per_feed]:\n",
        "            title = getattr(entry, \"title\", \"\") or \"\"\n",
        "            link = getattr(entry, \"link\", \"\") or \"\"\n",
        "            published = getattr(entry, \"published\", \"\") or getattr(entry, \"updated\", \"\") or \"\"\n",
        "            summary = getattr(entry, \"summary\", \"\") or getattr(entry, \"description\", \"\") or \"\"\n",
        "            item_id = _hash_id(feed_url, title, link)\n",
        "\n",
        "            items.append({\n",
        "                \"item_id\": item_id,\n",
        "                \"source\": \"rss\",\n",
        "                \"source_name\": feed_url,\n",
        "                \"title\": title,\n",
        "                \"url\": link,\n",
        "                \"published\": published,\n",
        "                \"content\": summary,\n",
        "                \"fetched_at\": dt.datetime.utcnow().isoformat(),\n",
        "            })\n",
        "    return items\n",
        "\n",
        "def fetch_newsapi_items(query: str, language: str = \"en\", page_size: int = 20) -> List[Dict[str, Any]]:\n",
        "    api_key = os.getenv(\"NEWSAPI_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"NEWSAPI_KEY not found. Set it in your .env file to use NewsAPI.\")\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    params = {\"q\": query, \"language\": language, \"pageSize\": page_size, \"sortBy\": \"publishedAt\"}\n",
        "    headers = {\"X-Api-Key\": api_key}\n",
        "    r = requests.get(url, params=params, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "\n",
        "    items: List[Dict[str, Any]] = []\n",
        "    for a in data.get(\"articles\", []):\n",
        "        title = a.get(\"title\", \"\") or \"\"\n",
        "        link = a.get(\"url\", \"\") or \"\"\n",
        "        published = a.get(\"publishedAt\", \"\") or \"\"\n",
        "        summary = (a.get(\"description\") or \"\") + \"\\n\" + (a.get(\"content\") or \"\")\n",
        "        source_name = (a.get(\"source\") or {}).get(\"name\", \"NewsAPI\")\n",
        "        item_id = _hash_id(\"newsapi\", title, link)\n",
        "\n",
        "        items.append({\n",
        "            \"item_id\": item_id,\n",
        "            \"source\": \"newsapi\",\n",
        "            \"source_name\": source_name,\n",
        "            \"title\": title,\n",
        "            \"url\": link,\n",
        "            \"published\": published,\n",
        "            \"content\": summary.strip(),\n",
        "            \"fetched_at\": dt.datetime.utcnow().isoformat(),\n",
        "        })\n",
        "    return items\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26435a5d",
      "metadata": {},
      "source": [
        "## 4) Storage: JSON\n",
        "Lightweight, structured, easy to demo. This aligns with the job’s requirement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b68d726",
      "metadata": {},
      "outputs": [],
      "source": [
        "# JSON storage helper functions\n",
        "\n",
        "def save_json(data, filename: str):\n",
        "    path = OUTPUT_DIR / filename\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "    return str(path)\n",
        "\n",
        "def save_text(text: str, filename: str):\n",
        "    path = OUTPUT_DIR / filename\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text or \"\")\n",
        "    return str(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5579e4a2",
      "metadata": {},
      "source": [
        "## 5) LLM Setup\n",
        "Keep it minimal: one OpenAI model is enough for this POC (you can swap later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7020f275",
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2413c96e",
      "metadata": {},
      "source": [
        "## 6) CrewAI Agents\n",
        "- Researcher → pulls/filters potential signals\n",
        "- Analyst → extracts structured fields + scoring\n",
        "- Writer → produces daily summary output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc23b999",
      "metadata": {},
      "outputs": [],
      "source": [
        "researcher_agent = Agent(\n",
        "    role=\"Deal Signal Researcher\",\n",
        "    goal=(\n",
        "        \"Identify items from the provided feed batch that may contain business/deal signals \"\n",
        "        \"(M&A, funding, partnerships, distress, regulatory).\"\n",
        "    ),\n",
        "    backstory=(\n",
        "        \"You are a business intelligence researcher. You quickly scan headlines and blurbs \"\n",
        "        \"and pick out items that are likely actionable deal signals.\"\n",
        "    ),\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    allow_delegation=False,\n",
        ")\n",
        "\n",
        "analysis_agent = Agent(\n",
        "    role=\"Deal Signal Analyst\",\n",
        "    goal=(\n",
        "        \"For each candidate item, extract structured deal signal fields, assign a relevance score, \"\n",
        "        \"and produce short rationale and tags.\"\n",
        "    ),\n",
        "    backstory=(\n",
        "        \"You are a disciplined analyst. You return structured JSON and keep explanations short, \"\n",
        "        \"explicit, and decision-ready.\"\n",
        "    ),\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    allow_delegation=False,\n",
        ")\n",
        "\n",
        "writer_agent = Agent(\n",
        "    role=\"Daily Deal Signal Reporter\",\n",
        "    goal=(\n",
        "        \"Write a concise daily summary of the highest scoring signals in a CLI/text-report style \"\n",
        "        \"for executives.\"\n",
        "    ),\n",
        "    backstory=(\n",
        "        \"You write crisp bullet summaries: what happened, why it matters, and confidence.\"\n",
        "    ),\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    allow_delegation=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2dca7ca",
      "metadata": {},
      "source": [
        "## 7) Tasks\n",
        "We keep tasks simple and deterministic:\n",
        "1) Filter down to likely candidates\n",
        "2) Produce structured JSON for each candidate\n",
        "3) Produce a daily summary for the run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f43b9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tasks(feed_batch: List[Dict[str, Any]], config: Dict[str, Any]):\n",
        "    criteria = {\n",
        "        \"industries\": config[\"industries\"],\n",
        "        \"signal_types\": config[\"signal_types\"],\n",
        "        \"min_relevance_score\": config[\"min_relevance_score\"],\n",
        "    }\n",
        "\n",
        "    batch_json = json.dumps(feed_batch, ensure_ascii=False)\n",
        "\n",
        "    # ---- Task 1: Research (shortlist item_ids) ----\n",
        "    research_desc = (\n",
        "        \"You will receive a JSON list of news/feed items.\\n\\n\"\n",
        "        \"Return ONLY a JSON list of item_id values that are likely to contain deal signals.\\n\"\n",
        "        \"Use these signal types: {signal_types}.\\n\"\n",
        "        \"Industries of interest: {industries}.\\n\\n\"\n",
        "        \"Items JSON:\\n\"\n",
        "    ).format(**criteria) + batch_json\n",
        "\n",
        "    research_task = Task(\n",
        "        description=research_desc,\n",
        "        expected_output=\"A JSON list of item_id strings.\",\n",
        "        agent=researcher_agent,\n",
        "        human_input=False,\n",
        "    )\n",
        "\n",
        "    # ---- Task 2: Analysis (structured extraction) ----\n",
        "    analysis_desc = (\n",
        "        \"Using the same feed items JSON and the shortlisted item_ids from the Researcher, \"\n",
        "        \"analyze each shortlisted item and return ONLY valid JSON (no markdown fences).\\n\\n\"\n",
        "        \"Return a JSON list where each element has:\\n\"\n",
        "        \"- item_id\\n\"\n",
        "        \"- signal_type (one of: {signal_types})\\n\"\n",
        "        \"- relevance_score (0.0 to 1.0)\\n\"\n",
        "        \"- tags (list of strings)\\n\"\n",
        "        \"- rationale (1-2 sentences)\\n\"\n",
        "        \"- llm_summary (2-3 bullets, plain text)\\n\\n\"\n",
        "        \"Scoring guidance:\\n\"\n",
        "        \"- 0.0-0.4: weak/irrelevant\\n\"\n",
        "        \"- 0.4-0.7: plausible but not strong\\n\"\n",
        "        \"- 0.7-1.0: strong actionable signal\\n\\n\"\n",
        "        \"Feed items JSON:\\n\"\n",
        "    ).format(**criteria) + batch_json\n",
        "\n",
        "    analysis_task = Task(\n",
        "        description=analysis_desc,\n",
        "        expected_output=\"A JSON list of structured deal-signal objects.\",\n",
        "        agent=analysis_agent,\n",
        "        human_input=False,\n",
        "        context=[research_task],\n",
        "    )\n",
        "\n",
        "    # ---- Task 3: Report ----\n",
        "    report_desc = (\n",
        "        \"Write a CLI-style daily summary for this run.\\n\\n\"\n",
        "        \"STRICT REQUIREMENTS:\\n\"\n",
        "        \"- Include ONLY items with relevance_score >= {min_relevance_score}\\n\"\n",
        "        \"- Sort items by relevance_score descending\\n\"\n",
        "        \"- Use the EXACT title and EXACT url provided in the input data\\n\"\n",
        "        \"- Do NOT invent, shorten, or replace URLs\\n\\n\"\n",
        "        \"Output format (follow exactly):\\n\"\n",
        "        \"[YYYY-MM-DD] DealSignal Scout Summary\\n\"\n",
        "        \"High Confidence Signals: N\\n\"\n",
        "        \"1) <Title> — <SignalType> (<Score>)\\n\"\n",
        "        \"   Reason: <1 sentence rationale>\\n\"\n",
        "        \"   URL: <full URL>\\n\"\n",
        "    ).format(**criteria)\n",
        "\n",
        "    report_task = Task(\n",
        "        description=report_desc,\n",
        "        expected_output=\"A plain-text daily summary report.\",\n",
        "        agent=writer_agent,\n",
        "        human_input=False,\n",
        "        context=[analysis_task],\n",
        "    )\n",
        "\n",
        "    return research_task, analysis_task, report_task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b271707",
      "metadata": {},
      "source": [
        "## 8) Run the POC\n",
        "This cell performs one on-demand run: fetch → analyze → store → print report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e616ba6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_poc(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # 1) Fetch\n",
        "    items = []\n",
        "    items.extend(fetch_rss_items(config[\"rss_feeds\"], max_items_per_feed=config[\"max_items_per_source\"]))\n",
        "\n",
        "    if config.get(\"use_newsapi\"):\n",
        "        items.extend(\n",
        "            fetch_newsapi_items(\n",
        "                query=config[\"newsapi_query\"],\n",
        "                language=config[\"newsapi_language\"],\n",
        "                page_size=config[\"newsapi_page_size\"],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Add run label to each item\n",
        "    for it in items:\n",
        "        it[\"run_label\"] = config[\"run_label\"]\n",
        "\n",
        "    # 2) CrewAI tasks\n",
        "    research_task, analysis_task, report_task = build_tasks(items, config)\n",
        "\n",
        "    crew = Crew(\n",
        "        agents=[researcher_agent, analysis_agent, writer_agent],\n",
        "        tasks=[research_task, analysis_task, report_task],\n",
        "        process=Process.sequential,\n",
        "        verbose=True,\n",
        "        interactive=False, # no Y/n prompts\n",
        "        enable_tracing=False, # disable tracing for POC (disables y/n prompt for \"would you like to view the trace?\")\n",
        "    )\n",
        "\n",
        "    _ = crew.kickoff()\n",
        "\n",
        "    # 3) Parse analysis JSON output (best-effort)\n",
        "    analysis_raw = getattr(analysis_task.output, \"raw\", None) if hasattr(analysis_task, \"output\") else None\n",
        "    if not analysis_raw:\n",
        "        analysis_raw = getattr(analysis_task, \"output\", None)\n",
        "\n",
        "    analysis_objects = []\n",
        "    if analysis_raw:\n",
        "        try:\n",
        "            analysis_objects = json.loads(str(analysis_raw))\n",
        "        except Exception:\n",
        "            # naive fallback: extract outermost JSON list\n",
        "            s = str(analysis_raw)\n",
        "            start = s.find(\"[\")\n",
        "            end = s.rfind(\"]\")\n",
        "            if start != -1 and end != -1 and end > start:\n",
        "                try:\n",
        "                    analysis_objects = json.loads(s[start : end + 1])\n",
        "                except Exception:\n",
        "                    analysis_objects = []\n",
        "\n",
        "    # 4) Normalize + collect structured outputs\n",
        "    by_id = {it[\"item_id\"]: it for it in items}\n",
        "    signals = []\n",
        "\n",
        "    for obj in analysis_objects:\n",
        "        item_id = obj.get(\"item_id\")\n",
        "        base = by_id.get(item_id)\n",
        "        if not base:\n",
        "            continue\n",
        "\n",
        "        row = {**base, **obj}\n",
        "\n",
        "        # Normalize fields defensively\n",
        "        row[\"relevance_score\"] = float(row.get(\"relevance_score\") or 0.0)\n",
        "\n",
        "        if isinstance(row.get(\"tags\"), list):\n",
        "            row[\"tags\"] = row[\"tags\"]\n",
        "        else:\n",
        "            row[\"tags\"] = []\n",
        "\n",
        "        if isinstance(row.get(\"llm_summary\"), list):\n",
        "            row[\"llm_summary\"] = \"\\n\".join([str(x) for x in row[\"llm_summary\"]])\n",
        "        else:\n",
        "            row[\"llm_summary\"] = str(row.get(\"llm_summary\", \"\"))\n",
        "\n",
        "        row[\"rationale\"] = str(row.get(\"rationale\", \"\"))\n",
        "\n",
        "        signals.append(row)\n",
        "\n",
        "    # 5) Export artifacts (outside the loop)\n",
        "    run_label = config[\"run_label\"]\n",
        "\n",
        "    raw_items_path = save_json(items, f\"raw_items_{run_label}.json\")\n",
        "    signals_path = save_json(signals, f\"signals_{run_label}.json\")\n",
        "\n",
        "    report_text = getattr(report_task.output, \"raw\", \"\")\n",
        "    summary_path = save_text(report_text, f\"summary_{run_label}.txt\")\n",
        "\n",
        "    return {\n",
        "        \"fetched_items\": len(items),\n",
        "        \"analyzed_items\": len(analysis_objects),\n",
        "        \"signals_exported\": len(signals),\n",
        "        \"raw_items_path\": raw_items_path,\n",
        "        \"signals_path\": signals_path,\n",
        "        \"summary_path\": summary_path,\n",
        "        \"report\": report_text,\n",
        "        \"run_label\": run_label,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e463c9ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "out = run_poc(CONFIG)\n",
        "print(out)\n",
        "print(\"\\n\" + out[\"report\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ced28165",
      "metadata": {},
      "source": [
        "## 9) Inspect Stored Results\n",
        "Quickly view what was saved for this run label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531a7092",
      "metadata": {},
      "outputs": [],
      "source": [
        "signals_path = out[\"signals_path\"]  # returned from run_poc()\n",
        "\n",
        "with open(signals_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    signals = json.load(f)\n",
        "\n",
        "print(f\"Signals exported for run {out['run_label']}: {len(signals)}\")\n",
        "\n",
        "for r in signals[:5]:\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{r['relevance_score']:.2f} | {r['signal_type']} | {r['title']}\")\n",
        "    print(f\"URL: {r['url']}\")\n",
        "    print(f\"Tags: {r.get('tags', [])}\")\n",
        "    print(f\"Rationale: {r.get('rationale', '')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef4fe4e4",
      "metadata": {},
      "source": [
        "## 10) Export Example Output (JSON)\n",
        "This gives you the “Example output from a real run” deliverable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed38095",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export signals again (optional manual export)\n",
        "export_path = OUTPUT_DIR / f\"dealsignal_output_{out['run_label']}.json\"\n",
        "\n",
        "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(signals, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Exported:\", str(export_path))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dlai001",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
